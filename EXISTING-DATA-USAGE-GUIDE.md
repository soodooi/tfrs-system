# ç°æœ‰æ•°æ®ä½¿ç”¨å’Œå¢é‡æ•°æ®å¤„ç†æŒ‡å—

> **é¡¹ç›®**: ManiDala POD Platform - TFRS Data Usage  
> **ç‰ˆæœ¬**: 1.0  
> **æ—¥æœŸ**: 2026-01-17  
> **ç›®æ ‡**: å……åˆ†åˆ©ç”¨ç°æœ‰æ•°æ®ï¼Œå»ºç«‹å¢é‡æ•°æ®å¤„ç†æœºåˆ¶

---

## ğŸ“‹ ç›®å½•

1. [ç°æœ‰æ•°æ®åˆ†æ](#ç°æœ‰æ•°æ®åˆ†æ)
2. [ç°æœ‰æ•°æ®æå–æ–¹æ¡ˆ](#ç°æœ‰æ•°æ®æå–æ–¹æ¡ˆ)
3. [å¢é‡æ•°æ®å¤„ç†](#å¢é‡æ•°æ®å¤„ç†)
4. [æ•°æ®åˆå¹¶ç­–ç•¥](#æ•°æ®åˆå¹¶ç­–ç•¥)
5. [å®æ–½æ­¥éª¤](#å®æ–½æ­¥éª¤)

---

## 1. ç°æœ‰æ•°æ®åˆ†æ

### 1.1 å¯ç”¨æ•°æ®æºæ¸…å•

æ ¹æ®æ•°æ®åº“ Schemaï¼Œæˆ‘ä»¬æœ‰ä»¥ä¸‹å¯ç”¨æ•°æ®ï¼š

| æ•°æ®è¡¨ | æ•°æ®é‡ä¼°ç®— | å¯ç”¨æ€§ | è®­ç»ƒä»·å€¼ |
|--------|-----------|--------|----------|
| **orders** | 100-500æ¡ | â­â­â­â­â­ | é«˜ï¼ˆè´­ä¹°è¡Œä¸ºï¼‰ |
| **base_products** | 2000-5000ä¸ª | â­â­â­â­â­ | é«˜ï¼ˆå•†å“ç‰¹å¾ï¼‰ |
| **end_products** | 500-2000ä¸ª | â­â­â­â­ | ä¸­ï¼ˆè®¾è®¡å¸ˆå…³è”ï¼‰ |
| **images** | 5000-10000ä¸ª | â­â­â­ | ä¸­ï¼ˆæµè§ˆè¡Œä¸ºï¼‰ |
| **accounts** | 500-1000ä¸ª | â­â­â­â­ | é«˜ï¼ˆç”¨æˆ·ç‰¹å¾ï¼‰ |

### 1.2 æ•°æ®è´¨é‡è¯„ä¼°

```sql
-- æ£€æŸ¥ç°æœ‰æ•°æ®é‡
SELECT 
  'orders' as table_name, COUNT(*) as count FROM orders
UNION ALL
SELECT 'base_products', COUNT(*) FROM base_products
UNION ALL
SELECT 'end_products', COUNT(*) FROM end_products
UNION ALL
SELECT 'images', COUNT(*) FROM images
UNION ALL
SELECT 'accounts', COUNT(*) FROM accounts;
```

---

## 2. ç°æœ‰æ•°æ®æå–æ–¹æ¡ˆ

### 2.1 ä»è®¢å•æ•°æ®æå–äº¤äº’

è®¢å•æ˜¯æœ€æœ‰ä»·å€¼çš„æ•°æ®æºï¼Œå› ä¸ºå®ƒä»£è¡¨çœŸå®çš„è´­ä¹°è¡Œä¸ºã€‚

#### A. æå–è´­ä¹°äº¤äº’

```sql
-- ä»è®¢å•ä¸­æå–ç”¨æˆ·-å•†å“äº¤äº’
SELECT 
  o.email as user_id,
  json_extract(item.value, '$.product_id') as item_id,
  5.0 as rating,  -- è´­ä¹°è¡Œä¸ºèµ‹äºˆæœ€é«˜æƒé‡
  strftime('%s', o.created_at) as timestamp,
  'purchase' as interaction_type,
  json_extract(item.value, '$.quantity') as quantity,
  json_extract(item.value, '$.price') as price
FROM orders o,
  json_each(o.line_items) as item
WHERE o.financial_status = 'paid'
  AND o.created_at IS NOT NULL
ORDER BY o.created_at;
```

#### B. ç”Ÿæˆæ¨¡æ‹Ÿæµè§ˆè¡Œä¸º

åŸºäºè´­ä¹°è®°å½•ï¼Œæˆ‘ä»¬å¯ä»¥åˆç†æ¨æ–­ç”¨æˆ·æµè§ˆäº†ç›¸å…³å•†å“ï¼š

```sql
-- ä¸ºæ¯ä¸ªè´­ä¹°ç”Ÿæˆæµè§ˆè¡Œä¸ºï¼ˆè´­ä¹°å‰å¿…ç„¶æµè§ˆï¼‰
SELECT 
  o.email as user_id,
  json_extract(item.value, '$.product_id') as item_id,
  3.0 as rating,  -- æµè§ˆè¡Œä¸ºæƒé‡è¾ƒä½
  strftime('%s', datetime(o.created_at, '-' || (RANDOM() % 3600 + 60) || ' seconds')) as timestamp,
  'view' as interaction_type
FROM orders o,
  json_each(o.line_items) as item
WHERE o.financial_status = 'paid';
```

#### C. ç”Ÿæˆç›¸ä¼¼å•†å“æµè§ˆ

ç”¨æˆ·è´­ä¹°æŸå•†å“æ—¶ï¼Œå¯èƒ½æµè§ˆäº†åŒç±»å•†å“ï¼š

```sql
-- ä¸ºæ¯ä¸ªè´­ä¹°ç”ŸæˆåŒç±»å•†å“æµè§ˆ
SELECT 
  o.email as user_id,
  bp.id as item_id,
  2.0 as rating,
  strftime('%s', datetime(o.created_at, '-' || (RANDOM() % 7200 + 300) || ' seconds')) as timestamp,
  'view' as interaction_type
FROM orders o,
  json_each(o.line_items) as purchased_item,
  base_products bp
WHERE o.financial_status = 'paid'
  AND bp.category_id = (
    SELECT category_id 
    FROM base_products 
    WHERE id = json_extract(purchased_item.value, '$.product_id')
  )
  AND bp.id != json_extract(purchased_item.value, '$.product_id')
  AND RANDOM() % 100 < 30  -- 30% æ¦‚ç‡ç”Ÿæˆ
LIMIT 3;  -- æ¯ä¸ªè´­ä¹°æœ€å¤š3ä¸ªç›¸ä¼¼å•†å“
```

### 2.2 ä»å›¾ç‰‡æµè§ˆæ•°æ®æå–äº¤äº’

```sql
-- ä»å›¾ç‰‡æµè§ˆè®°å½•æå–äº¤äº’
SELECT 
  'anonymous_' || CAST(RANDOM() AS TEXT) as user_id,  -- åŒ¿åç”¨æˆ·
  entity_id as item_id,
  1.0 as rating,  -- å›¾ç‰‡æµè§ˆæƒé‡æœ€ä½
  strftime('%s', created_at) as timestamp,
  'image_view' as interaction_type,
  view_count
FROM images
WHERE type = 'design'
  AND entity_id IS NOT NULL
  AND view_count > 0;
```

### 2.3 å®Œæ•´æ•°æ®æå–è„šæœ¬

åˆ›å»ºä¸€ä¸ª Worker æ¥æå–å’Œè½¬æ¢ç°æœ‰æ•°æ®ï¼š

```javascript
// workers/manidala-data-extract/src/index.js
export default {
  async fetch(request, env) {
    const url = new URL(request.url);
    
    if (url.pathname === '/extract/existing-data') {
      return await extractExistingData(env);
    }
    
    return new Response('Not found', { status: 404 });
  }
};

async function extractExistingData(env) {
  const interactions = [];
  
  // 1. æå–è´­ä¹°äº¤äº’
  const purchases = await env.DB.prepare(`
    SELECT 
      o.email as user_id,
      json_extract(item.value, '$.product_id') as item_id,
      5.0 as rating,
      strftime('%s', o.created_at) as timestamp,
      'purchase' as interaction_type
    FROM orders o,
      json_each(o.line_items) as item
    WHERE o.financial_status = 'paid'
      AND o.created_at IS NOT NULL
  `).all();
  
  interactions.push(...purchases.results);
  
  // 2. ç”Ÿæˆæµè§ˆè¡Œä¸ºï¼ˆè´­ä¹°å‰ï¼‰
  const views = await env.DB.prepare(`
    SELECT 
      o.email as user_id,
      json_extract(item.value, '$.product_id') as item_id,
      3.0 as rating,
      strftime('%s', datetime(o.created_at, '-' || (ABS(RANDOM()) % 3600 + 60) || ' seconds')) as timestamp,
      'view' as interaction_type
    FROM orders o,
      json_each(o.line_items) as item
    WHERE o.financial_status = 'paid'
  `).all();
  
  interactions.push(...views.results);
  
  // 3. æ’å…¥åˆ°è®­ç»ƒäº¤äº’è¡¨
  const stmt = env.DB.prepare(`
    INSERT INTO training_interactions (
      user_id, item_id, rating, timestamp, interaction_type, source
    ) VALUES (?, ?, ?, ?, ?, 'existing_data')
  `);
  
  const batch = interactions.map(i => 
    stmt.bind(i.user_id, i.item_id, i.rating, i.timestamp, i.interaction_type)
  );
  
  await env.DB.batch(batch);
  
  // 4. å¯¼å‡ºä¸º CSV
  const csv = convertToCSV(interactions);
  
  // 5. ä¸Šä¼ åˆ° R2
  await env.R2_BUCKET.put(
    `training-data/existing-data-${Date.now()}.csv`,
    csv,
    {
      httpMetadata: {
        contentType: 'text/csv'
      }
    }
  );
  
  return new Response(JSON.stringify({
    success: true,
    total_interactions: interactions.length,
    breakdown: {
      purchases: purchases.results.length,
      views: views.results.length
    }
  }), {
    headers: { 'Content-Type': 'application/json' }
  });
}

function convertToCSV(data) {
  const headers = ['user_id', 'item_id', 'rating', 'timestamp', 'interaction_type'];
  const rows = data.map(row => 
    headers.map(h => row[h]).join(',')
  );
  return [headers.join(','), ...rows].join('\n');
}
```

---

## 3. å¢é‡æ•°æ®å¤„ç†

### 3.1 å¢é‡æ•°æ®æ”¶é›†ç­–ç•¥

é‡‡ç”¨**åŒè½¨åˆ¶**ï¼šç°æœ‰æ•°æ® + å®æ—¶å¢é‡æ•°æ®

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           æ•°æ®æ”¶é›†åŒè½¨åˆ¶                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  è½¨é“ 1: ç°æœ‰æ•°æ®ï¼ˆä¸€æ¬¡æ€§ï¼‰                  â”‚
â”‚  â”œâ”€ è®¢å•æ•°æ®æå–                            â”‚
â”‚  â”œâ”€ å›¾ç‰‡æµè§ˆæå–                            â”‚
â”‚  â””â”€ ç”Ÿæˆæ¨¡æ‹Ÿè¡Œä¸º                            â”‚
â”‚                                             â”‚
â”‚  è½¨é“ 2: å¢é‡æ•°æ®ï¼ˆæŒç»­ï¼‰                    â”‚
â”‚  â”œâ”€ å‰ç«¯åŸ‹ç‚¹æ”¶é›†                            â”‚
â”‚  â”œâ”€ API æ—¥å¿—æ”¶é›†                            â”‚
â”‚  â””â”€ æ¨èåé¦ˆæ”¶é›†                            â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 å¢é‡æ•°æ®è‡ªåŠ¨åŒæ­¥

#### A. è®¢å•å¢é‡åŒæ­¥

```javascript
// ç›‘å¬ Shoplazza Webhook - æ–°è®¢å•
export default {
  async fetch(request, env) {
    if (request.method !== 'POST') {
      return new Response('Method not allowed', { status: 405 });
    }
    
    const webhook = await request.json();
    
    if (webhook.topic === 'orders/paid') {
      await processNewOrder(webhook.data, env);
    }
    
    return new Response('OK');
  }
};

async function processNewOrder(order, env) {
  // 1. ä¿å­˜è®¢å•åˆ°æ•°æ®åº“
  await env.DB.prepare(`
    INSERT INTO orders (shoplazza_id, email, line_items, created_at, ...)
    VALUES (?, ?, ?, ?, ...)
  `).bind(order.id, order.email, JSON.stringify(order.line_items), ...).run();
  
  // 2. ç«‹å³ç”Ÿæˆè®­ç»ƒäº¤äº’
  const interactions = [];
  
  for (const item of order.line_items) {
    // è´­ä¹°äº¤äº’
    interactions.push({
      user_id: order.email,
      item_id: item.product_id,
      rating: 5.0,
      timestamp: Math.floor(Date.now() / 1000),
      interaction_type: 'purchase'
    });
    
    // æ¨¡æ‹Ÿæµè§ˆäº¤äº’ï¼ˆè´­ä¹°å‰5-60åˆ†é’Ÿï¼‰
    interactions.push({
      user_id: order.email,
      item_id: item.product_id,
      rating: 3.0,
      timestamp: Math.floor(Date.now() / 1000) - (Math.random() * 3300 + 300),
      interaction_type: 'view'
    });
  }
  
  // 3. æ’å…¥è®­ç»ƒäº¤äº’è¡¨
  const stmt = env.DB.prepare(`
    INSERT INTO training_interactions (
      user_id, item_id, rating, timestamp, interaction_type, source
    ) VALUES (?, ?, ?, ?, ?, 'webhook')
  `);
  
  const batch = interactions.map(i => 
    stmt.bind(i.user_id, i.item_id, i.rating, i.timestamp, i.interaction_type)
  );
  
  await env.DB.batch(batch);
  
  // 4. è§¦å‘å¢é‡è®­ç»ƒï¼ˆå¦‚æœç´¯ç§¯è¶³å¤Ÿæ•°æ®ï¼‰
  await checkAndTriggerIncrementalTraining(env);
}
```

#### B. å‰ç«¯è¡Œä¸ºå¢é‡åŒæ­¥

å‰ç«¯ SDK å®æ—¶æ”¶é›†çš„æ•°æ®ä¼šè‡ªåŠ¨è¿›å…¥ `user_behaviors` è¡¨ï¼Œå®šæœŸè½¬æ¢ä¸ºè®­ç»ƒæ•°æ®ï¼š

```javascript
// Scheduled Worker - æ¯å°æ—¶æ‰§è¡Œ
export default {
  async scheduled(event, env) {
    // 1. æŸ¥è¯¢æœ€è¿‘1å°æ—¶çš„æ–°è¡Œä¸º
    const lastSync = await env.KV.get('last_sync_timestamp') || 
                     Math.floor(Date.now() / 1000) - 3600;
    
    const newBehaviors = await env.DB.prepare(`
      SELECT 
        COALESCE(user_id, session_id) as user_id,
        product_id as item_id,
        CASE event_type
          WHEN 'purchase' THEN 5.0
          WHEN 'add_to_cart' THEN 4.0
          WHEN 'add_to_wishlist' THEN 3.0
          WHEN 'product_click' THEN 2.0
          WHEN 'product_view' THEN 1.0
          ELSE 0.5
        END as rating,
        strftime('%s', timestamp) as timestamp,
        event_type as interaction_type
      FROM user_behaviors
      WHERE strftime('%s', timestamp) > ?
        AND product_id IS NOT NULL
      ORDER BY timestamp
    `).bind(lastSync).all();
    
    if (newBehaviors.results.length === 0) {
      console.log('No new behaviors to sync');
      return;
    }
    
    // 2. æ’å…¥è®­ç»ƒäº¤äº’è¡¨
    const stmt = env.DB.prepare(`
      INSERT INTO training_interactions (
        user_id, item_id, rating, timestamp, interaction_type, source
      ) VALUES (?, ?, ?, ?, ?, 'realtime')
    `);
    
    const batch = newBehaviors.results.map(b => 
      stmt.bind(b.user_id, b.item_id, b.rating, b.timestamp, b.interaction_type)
    );
    
    await env.DB.batch(batch);
    
    // 3. æ›´æ–°åŒæ­¥æ—¶é—´æˆ³
    await env.KV.put('last_sync_timestamp', 
                     Math.floor(Date.now() / 1000).toString());
    
    console.log(`Synced ${newBehaviors.results.length} new behaviors`);
    
    // 4. æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡è®­ç»ƒ
    await checkAndTriggerIncrementalTraining(env);
  }
};
```

### 3.3 å¢é‡è®­ç»ƒè§¦å‘æœºåˆ¶

```javascript
async function checkAndTriggerIncrementalTraining(env) {
  // 1. æ£€æŸ¥è‡ªä¸Šæ¬¡è®­ç»ƒä»¥æ¥çš„æ–°æ•°æ®é‡
  const lastTraining = await env.KV.get('last_training_timestamp') || 0;
  
  const newDataCount = await env.DB.prepare(`
    SELECT COUNT(*) as count
    FROM training_interactions
    WHERE created_at > datetime(?, 'unixepoch')
  `).bind(lastTraining).first();
  
  // 2. åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è®­ç»ƒ
  const RETRAIN_THRESHOLD = 10000;  // ç´¯ç§¯1ä¸‡æ¡æ–°æ•°æ®
  const RETRAIN_INTERVAL = 7 * 24 * 3600;  // æˆ–7å¤©
  
  const shouldRetrain = 
    newDataCount.count >= RETRAIN_THRESHOLD ||
    (Date.now() / 1000 - lastTraining) >= RETRAIN_INTERVAL;
  
  if (shouldRetrain) {
    // 3. å¯¼å‡ºå¢é‡æ•°æ®
    await exportIncrementalData(env, lastTraining);
    
    // 4. è§¦å‘è®­ç»ƒé€šçŸ¥ï¼ˆå¯ä»¥æ˜¯é‚®ä»¶ã€Slackç­‰ï¼‰
    await notifyRetraining(env, {
      new_data_count: newDataCount.count,
      last_training: new Date(lastTraining * 1000).toISOString()
    });
    
    // 5. æ›´æ–°è®­ç»ƒæ—¶é—´æˆ³
    await env.KV.put('last_training_timestamp', 
                     Math.floor(Date.now() / 1000).toString());
  }
}

async function exportIncrementalData(env, since) {
  const data = await env.DB.prepare(`
    SELECT user_id, item_id, rating, timestamp, interaction_type
    FROM training_interactions
    WHERE created_at > datetime(?, 'unixepoch')
    ORDER BY timestamp
  `).bind(since).all();
  
  const csv = convertToCSV(data.results);
  
  await env.R2_BUCKET.put(
    `training-data/incremental-${Date.now()}.csv`,
    csv,
    {
      httpMetadata: {
        contentType: 'text/csv'
      },
      customMetadata: {
        type: 'incremental',
        since: since.toString(),
        count: data.results.length.toString()
      }
    }
  );
}
```

---

## 4. æ•°æ®åˆå¹¶ç­–ç•¥

### 4.1 è®­ç»ƒæ•°æ®åˆå¹¶æ–¹æ¡ˆ

#### æ–¹æ¡ˆ A: å…¨é‡é‡è®­ç»ƒï¼ˆæ¨èï¼‰

```python
# åœ¨ Colab ä¸­åˆå¹¶æ‰€æœ‰æ•°æ®
import pandas as pd
from google.colab import drive

# 1. æŒ‚è½½ Google Drive
drive.mount('/content/drive')

# 2. è¯»å–æ‰€æœ‰æ•°æ®æ–‡ä»¶
existing_data = pd.read_csv('/content/drive/MyDrive/tfrs-data/existing-data.csv')
incremental_data = pd.read_csv('/content/drive/MyDrive/tfrs-data/incremental-*.csv')

# 3. åˆå¹¶æ•°æ®
all_data = pd.concat([existing_data, incremental_data], ignore_index=True)

# 4. å»é‡ï¼ˆåŒä¸€ç”¨æˆ·-å•†å“-æ—¶é—´çª—å£ï¼‰
all_data['time_window'] = pd.to_datetime(all_data['timestamp'], unit='s').dt.floor('5min')
all_data = all_data.drop_duplicates(
    subset=['user_id', 'item_id', 'time_window'],
    keep='last'
)

# 5. æŒ‰æ—¶é—´æ’åº
all_data = all_data.sort_values('timestamp')

# 6. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†ï¼ˆ80/20ï¼‰
split_idx = int(len(all_data) * 0.8)
train_data = all_data[:split_idx]
test_data = all_data[split_idx:]

print(f"Total: {len(all_data)}, Train: {len(train_data)}, Test: {len(test_data)}")
```

#### æ–¹æ¡ˆ B: å¢é‡è®­ç»ƒï¼ˆé«˜çº§ï¼‰

```python
# åŠ è½½å·²æœ‰æ¨¡å‹ï¼Œä½¿ç”¨æ–°æ•°æ®ç»§ç»­è®­ç»ƒ
import tensorflow as tf
import tensorflow_recommenders as tfrs

# 1. åŠ è½½å·²æœ‰æ¨¡å‹
model = tf.keras.models.load_model('/content/drive/MyDrive/tfrs-models/model-v1')

# 2. å‡†å¤‡å¢é‡æ•°æ®
incremental_dataset = tf.data.Dataset.from_tensor_slices({
    'user_id': incremental_data['user_id'].values,
    'item_id': incremental_data['item_id'].values,
    'rating': incremental_data['rating'].values
})

# 3. ç»§ç»­è®­ç»ƒï¼ˆè¾ƒå°‘çš„ epochsï¼‰
model.fit(
    incremental_dataset.batch(256),
    epochs=5,  # å¢é‡è®­ç»ƒç”¨è¾ƒå°‘ epochs
    verbose=1
)

# 4. ä¿å­˜æ–°ç‰ˆæœ¬æ¨¡å‹
model.save('/content/drive/MyDrive/tfrs-models/model-v2')
```

### 4.2 æ•°æ®ç‰ˆæœ¬ç®¡ç†

```javascript
// åœ¨ R2 ä¸­ç®¡ç†æ•°æ®ç‰ˆæœ¬
const DATA_VERSIONS = {
  'v1.0': {
    files: ['existing-data-20260117.csv'],
    total_interactions: 5000,
    date: '2026-01-17'
  },
  'v1.1': {
    files: ['existing-data-20260117.csv', 'incremental-20260124.csv'],
    total_interactions: 15000,
    date: '2026-01-24'
  },
  'v2.0': {
    files: ['merged-data-20260201.csv'],
    total_interactions: 50000,
    date: '2026-02-01'
  }
};

// ä¿å­˜ç‰ˆæœ¬ä¿¡æ¯åˆ° KV
await env.KV.put('data_versions', JSON.stringify(DATA_VERSIONS));
```

---

## 5. å®æ–½æ­¥éª¤

### 5.1 ç¬¬ä¸€æ­¥ï¼šæå–ç°æœ‰æ•°æ®ï¼ˆç«‹å³æ‰§è¡Œï¼‰

```bash
# 1. éƒ¨ç½²æ•°æ®æå– Worker
cd workers/manidala-data-extract
wrangler deploy

# 2. è§¦å‘æ•°æ®æå–
curl -X POST https://data-extract.your-domain.workers.dev/extract/existing-data

# 3. ä¸‹è½½ç”Ÿæˆçš„ CSV
# ä» R2 ä¸‹è½½: training-data/existing-data-*.csv
```

### 5.2 ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ç°æœ‰æ•°æ®è®­ç»ƒåˆå§‹æ¨¡å‹ï¼ˆ1-2å¤©ï¼‰

```python
# åœ¨ Colab ä¸­
# 1. ä¸Šä¼  existing-data.csv åˆ° Google Drive
# 2. è¿è¡Œè®­ç»ƒç¬”è®°æœ¬
# 3. ä¸‹è½½è®­ç»ƒå¥½çš„æ¨¡å‹
```

### 5.3 ç¬¬ä¸‰æ­¥ï¼šéƒ¨ç½²å¢é‡æ•°æ®æ”¶é›†ï¼ˆ1å‘¨ï¼‰

```bash
# 1. éƒ¨ç½²å‰ç«¯ Tracking SDK
# 2. éƒ¨ç½² Tracking Worker
# 3. é…ç½® Scheduled Workerï¼ˆæ¯å°æ—¶åŒæ­¥ï¼‰
# 4. é…ç½® Webhookï¼ˆè®¢å•å®æ—¶åŒæ­¥ï¼‰
```

### 5.4 ç¬¬å››æ­¥ï¼šå»ºç«‹å¢é‡è®­ç»ƒæµç¨‹ï¼ˆæŒç»­ï¼‰

```
æ¯å‘¨æµç¨‹ï¼š
1. å‘¨ä¸€ï¼šæ£€æŸ¥ç´¯ç§¯æ•°æ®é‡
2. å‘¨äºŒï¼šå¦‚æœ > 1ä¸‡æ¡ï¼Œå¯¼å‡ºå¢é‡æ•°æ®
3. å‘¨ä¸‰ï¼šåœ¨ Colab é‡è®­ç»ƒæ¨¡å‹
4. å‘¨å››ï¼šéƒ¨ç½²æ–°æ¨¡å‹åˆ° Railway
5. å‘¨äº”ï¼šA/B æµ‹è¯•æ–°æ—§æ¨¡å‹
```

---

## 6. æ•°æ®é‡é¢„ä¼°

### 6.1 ç°æœ‰æ•°æ®é¢„ä¼°

| æ•°æ®æº | é¢„ä¼°é‡ | è®­ç»ƒäº¤äº’æ•° |
|--------|--------|-----------|
| è®¢å•ï¼ˆè´­ä¹°ï¼‰ | 100-500 | 100-500 |
| è®¢å•ï¼ˆæ¨¡æ‹Ÿæµè§ˆï¼‰ | 100-500 | 100-500 |
| è®¢å•ï¼ˆç›¸ä¼¼å•†å“ï¼‰ | 300-1500 | 300-1500 |
| å›¾ç‰‡æµè§ˆ | 5000-10000 | 5000-10000 |
| **æ€»è®¡** | | **5500-12500** |

### 6.2 å¢é‡æ•°æ®é¢„ä¼°

å‡è®¾æ—¥æ´» 100 ç”¨æˆ·ï¼Œæ¯ç”¨æˆ· 10 æ¬¡äº¤äº’ï¼š

| æ—¶é—´ | æ—¥å¢é‡ | ç´¯ç§¯é‡ | æ˜¯å¦é‡è®­ç»ƒ |
|------|--------|--------|-----------|
| ç¬¬1å‘¨ | 1000 | 6500-13500 | âŒ |
| ç¬¬2å‘¨ | 1000 | 13500-20500 | âœ… |
| ç¬¬4å‘¨ | 1000 | 27500-34500 | âœ… |
| ç¬¬8å‘¨ | 1000 | 55500-62500 | âœ… |

---

## 7. ç›‘æ§å’Œä¼˜åŒ–

### 7.1 æ•°æ®è´¨é‡ç›‘æ§

```javascript
// æ¯æ—¥æ•°æ®è´¨é‡æŠ¥å‘Š
export default {
  async scheduled(event, env) {
    const report = {
      date: new Date().toISOString().split('T')[0],
      existing_data: {},
      incremental_data: {},
      total: {}
    };
    
    // ç°æœ‰æ•°æ®ç»Ÿè®¡
    const existing = await env.DB.prepare(`
      SELECT 
        COUNT(*) as count,
        COUNT(DISTINCT user_id) as users,
        COUNT(DISTINCT item_id) as items
      FROM training_interactions
      WHERE source = 'existing_data'
    `).first();
    
    report.existing_data = existing;
    
    // å¢é‡æ•°æ®ç»Ÿè®¡
    const incremental = await env.DB.prepare(`
      SELECT 
        COUNT(*) as count,
        COUNT(DISTINCT user_id) as users,
        COUNT(DISTINCT item_id) as items
      FROM training_interactions
      WHERE source IN ('webhook', 'realtime')
    `).first();
    
    report.incremental_data = incremental;
    
    // æ€»è®¡
    report.total = {
      count: existing.count + incremental.count,
      users: existing.users + incremental.users,
      items: existing.items + incremental.items
    };
    
    // ä¿å­˜æŠ¥å‘Š
    await env.R2_BUCKET.put(
      `reports/data-quality-${report.date}.json`,
      JSON.stringify(report, null, 2)
    );
    
    console.log('Data quality report:', report);
  }
};
```

### 7.2 æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **æ‰¹é‡å¤„ç†**ï¼šæ¯å°æ—¶æ‰¹é‡åŒæ­¥ï¼Œè€Œéå®æ—¶
2. **æ•°æ®å‹ç¼©**ï¼šä½¿ç”¨ Parquet æ ¼å¼å­˜å‚¨å¤§æ•°æ®
3. **å¢é‡å¯¼å‡º**ï¼šåªå¯¼å‡ºæ–°å¢æ•°æ®ï¼Œå‡å°‘ä¼ è¾“
4. **å®šæœŸæ¸…ç†**ï¼šåˆ é™¤1å¹´ä»¥ä¸Šçš„æ—§æ•°æ®

---

## 8. å¸¸è§é—®é¢˜

### Q1: ç°æœ‰æ•°æ®å¤ªå°‘æ€ä¹ˆåŠï¼Ÿ

**A**: é‡‡ç”¨æ··åˆç­–ç•¥ï¼š
- ä½¿ç”¨å…¬å¼€æ•°æ®é›†ï¼ˆMovieLensï¼‰é¢„è®­ç»ƒ
- ç”¨ç°æœ‰æ•°æ®å¾®è°ƒï¼ˆFine-tuningï¼‰
- ç”Ÿæˆåˆç†çš„æ¨¡æ‹Ÿæ•°æ®è¡¥å……

### Q2: å¦‚ä½•å¤„ç†åŒ¿åç”¨æˆ·ï¼Ÿ

**A**: ä½¿ç”¨ session_id æˆ– device_id ä½œä¸ºä¸´æ—¶ç”¨æˆ·IDï¼š
```sql
SELECT COALESCE(user_id, session_id, device_id) as user_id
```

### Q3: å¢é‡æ•°æ®ä½•æ—¶è§¦å‘é‡è®­ç»ƒï¼Ÿ

**A**: ä¸‰ä¸ªæ¡ä»¶ä¹‹ä¸€æ»¡è¶³å³å¯ï¼š
- æ–°æ•°æ®é‡ > 10000 æ¡
- è·ä¸Šæ¬¡è®­ç»ƒ > 7 å¤©
- æ¨¡å‹æ€§èƒ½ä¸‹é™ > 10%

### Q4: å¦‚ä½•é¿å…æ•°æ®é‡å¤ï¼Ÿ

**A**: ä½¿ç”¨å”¯ä¸€çº¦æŸå’Œå»é‡é€»è¾‘ï¼š
```sql
CREATE UNIQUE INDEX idx_unique_interaction 
ON training_interactions(user_id, item_id, timestamp);
```

---

## 9. æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **ç°æœ‰æ•°æ®**ï¼šç«‹å³æå–è®¢å•å’Œæµè§ˆæ•°æ®ï¼Œå¯è·å¾— 5000-12000 æ¡äº¤äº’
2. **å¢é‡æ•°æ®**ï¼šé€šè¿‡å‰ç«¯åŸ‹ç‚¹å’Œ Webhook æŒç»­æ”¶é›†
3. **æ•°æ®åˆå¹¶**ï¼šå®šæœŸï¼ˆæ¯å‘¨ï¼‰åˆå¹¶å…¨é‡æ•°æ®é‡è®­ç»ƒ
4. **ç‰ˆæœ¬ç®¡ç†**ï¼šä½¿ç”¨ R2 + KV ç®¡ç†æ•°æ®å’Œæ¨¡å‹ç‰ˆæœ¬

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. âœ… **ä»Šå¤©**ï¼šè¿è¡Œæ•°æ®æå–è„šæœ¬ï¼Œè·å–ç°æœ‰æ•°æ®
2. âœ… **æ˜å¤©**ï¼šä½¿ç”¨ç°æœ‰æ•°æ®è®­ç»ƒç¬¬ä¸€ä¸ªæ¨¡å‹
3. âœ… **æœ¬å‘¨**ï¼šéƒ¨ç½²å¢é‡æ•°æ®æ”¶é›†ç³»ç»Ÿ
4. âœ… **ä¸‹å‘¨**ï¼šå»ºç«‹è‡ªåŠ¨åŒ–è®­ç»ƒæµç¨‹

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**åˆ›å»ºæ—¥æœŸ**: 2026-01-17  
**ç»´æŠ¤è€…**: ManiDala Team  
**çŠ¶æ€**: âœ… å¯æ‰§è¡Œ
